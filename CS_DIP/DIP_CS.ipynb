{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DIP_CS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYc0Vsnt_1L4",
        "outputId": "72dc21db-a9d1-42ee-e507-b9117890c55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6p3sfvp_A_S",
        "outputId": "3255b825-e7af-45d3-c6fb-0074917a4618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'compsensing_dip'...\n",
            "remote: Enumerating objects: 9970, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 9970 (delta 0), reused 3 (delta 0), pack-reused 9967\u001b[K\n",
            "Receiving objects: 100% (9970/9970), 839.96 MiB | 30.04 MiB/s, done.\n",
            "Resolving deltas: 100% (1336/1336), done.\n",
            "Checking out files: 100% (5223/5223), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/davevanveen/compsensing_dip.git\n",
        "!cd compsensing_dip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r 'compsensing_dip/requirements.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w4KZTng_aBi",
        "outputId": "a5796da4-874b-4b91-db58-fcb3b0dd626f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python==2.7 (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for python==2.7\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#utils\n",
        "import numpy as np\n",
        "import os\n",
        "import errno\n",
        "import parser\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import datasets,transforms\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "class DCGAN_XRAY(nn.Module):\n",
        "    def __init__(self, nz, ngf=64, output_size=256, nc=3, num_measurements=1000):\n",
        "        super(DCGAN_XRAY, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(nz, ngf, 4, 1, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(ngf)\n",
        "        self.conv2 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(ngf)\n",
        "        self.conv3 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(ngf)\n",
        "        self.conv4 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(ngf)\n",
        "        self.conv5 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(ngf)\n",
        "        self.conv6 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn6 = nn.BatchNorm2d(ngf)\n",
        "        self.conv7 = nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False) #output is image\n",
        "    \n",
        "    def forward(self, z):\n",
        "        input_size = z.size()\n",
        "        x = F.relu(self.bn1(self.conv1(z)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "        x = torch.tanh(self.conv7(x,output_size=(-1,self.nc,self.output_size,self.output_size)))\n",
        "       \n",
        "        return x\n",
        "\n",
        "class DCGAN_MNIST(nn.Module):\n",
        "    def __init__(self, nz, ngf=64, output_size=28, nc=1, num_measurements=10):\n",
        "        super(DCGAN_MNIST, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(nz, ngf*8, 2, 1, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(ngf*8)\n",
        "        self.conv2 = nn.ConvTranspose2d(ngf*8, ngf*4, 4, 1, 0, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(ngf*4)\n",
        "        self.conv3 = nn.ConvTranspose2d(ngf*4, ngf*2, 3, 1, 1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(ngf*2)\n",
        "        self.conv4 = nn.ConvTranspose2d(ngf*2, ngf, 3, 1, 1, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(ngf)\n",
        "        self.conv5 = nn.ConvTranspose2d(ngf, nc, 3, 1, 1, bias=False) \n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        input_size = x.size()\n",
        "\n",
        "        # DCGAN_MNIST with old PyTorch version\n",
        "        # x = F.upsample(F.relu(self.bn1(self.conv1(x))),scale_factor=2)\n",
        "        # x = F.relu(self.bn2(self.conv2(x)))\n",
        "        # x = F.upsample(F.relu(self.bn3(self.conv3(x))),scale_factor=2)\n",
        "        # x = F.upsample(F.relu(self.bn4(self.conv4(x))),scale_factor=2)\n",
        "        # x = torch.tanh(self.conv5(x,output_size=(-1,self.nc,self.output_size,self.output_size)))\n",
        "\n",
        "        x = F.interpolate(F.relu(self.bn1(self.conv1(x))),scale_factor=2)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.interpolate(F.relu(self.bn3(self.conv3(x))),scale_factor=2)\n",
        "        x = F.interpolate(F.relu(self.bn4(self.conv4(x))),scale_factor=2)\n",
        "        x = torch.tanh(self.conv5(x,output_size=(-1,self.nc,self.output_size,self.output_size)))\n",
        "       \n",
        "        return x\n",
        "\n",
        "class DCGAN_RETINO(nn.Module):\n",
        "    def __init__(self, nz, ngf=64, output_size=256, nc=3, num_measurements=1000):\n",
        "        super(DCGAN_RETINO, self).__init__()\n",
        "        self.nc = nc\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.conv1 = nn.ConvTranspose2d(nz, ngf, 4, 1, 0, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(ngf)\n",
        "        self.conv2 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(ngf)\n",
        "        self.conv3 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(ngf)\n",
        "        self.conv4 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn4 = nn.BatchNorm2d(ngf)\n",
        "        self.conv5 = nn.ConvTranspose2d(ngf, ngf, 6, 2, 2, bias=False)\n",
        "        self.bn5 = nn.BatchNorm2d(ngf)\n",
        "        self.conv6 = nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False)\n",
        "        #self.fc = nn.Linear((output_size)*(output_size)*nc,num_measurements, bias=False) #fc layer - old version\n",
        "   \n",
        "    def forward(self, x):\n",
        "        input_size = x.size()\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = torch.tanh(self.conv6(x,output_size=(-1,self.nc,self.output_size,self.output_size)))\n",
        "       \n",
        "        return x\n",
        "\n",
        "NGF = 64\n",
        "def init_dcgan(args):\n",
        "\n",
        "    if args.DATASET == 'xray':\n",
        "        net = DCGAN_XRAY(args.Z_DIM, NGF, args.IMG_SIZE,\\\n",
        "            args.NUM_CHANNELS, args.NUM_MEASUREMENTS)\n",
        "    elif args.DATASET == 'mnist':\n",
        "        net = DCGAN_MNIST(args.Z_DIM, NGF, args.IMG_SIZE,\\\n",
        "            args.NUM_CHANNELS, args.NUM_MEASUREMENTS)\n",
        "    elif args.DATASET == 'retino':\n",
        "        net = DCGAN_RETINO(args.Z_DIM, NGF, args.IMG_SIZE,\\\n",
        "            args.NUM_CHANNELS, args.NUM_MEASUREMENTS)\n",
        "    return net\n",
        "\n",
        "def init_output_arrays(args):\n",
        "    loss_re = np.zeros((args.NUM_RESTARTS, BATCH_SIZE))\n",
        "    recons_re = np.zeros((args.NUM_RESTARTS, BATCH_SIZE, args.NUM_CHANNELS, \\\n",
        "                    args.IMG_SIZE, args.IMG_SIZE))\n",
        "    return loss_re, recons_re\n",
        "\n",
        "lambdas_tv = {'mnist': 1e-2, 'xray': 5e-2, 'retino': 2e-2}\n",
        "lambdas_lr = {'mnist': 0, 'xray': 100, 'retino': 1000}\n",
        "def get_constants(args, dtype):\n",
        "    MU_FN = 'mu_{0}.npy'.format(args.NUM_MEASUREMENTS)\n",
        "    MU_PATH = os.path.join(args.LR_FOLDER,MU_FN)\n",
        "    SIG_FN = \"sig_{0}.npy\".format(args.NUM_MEASUREMENTS)\n",
        "    SIG_PATH = os.path.join(args.LR_FOLDER,SIG_FN)\n",
        "    mu_ = np.load(MU_PATH)\n",
        "    sig_ = np.load(SIG_PATH)\n",
        "\n",
        "    mu = torch.FloatTensor(mu_).type(dtype)\n",
        "    sig_inv = torch.FloatTensor(np.linalg.inv(sig_)).type(dtype)\n",
        "    try:\n",
        "        tvc = lambdas_tv[args.DATASET]\n",
        "    except AttributeError:\n",
        "        tvc = 1e-2\n",
        "    try:\n",
        "        lrc = lambdas_lr[args.DATASET]\n",
        "    except AttributeError:\n",
        "        lrc = 0\n",
        "    return mu, sig_inv, tvc, lrc\n",
        "\n",
        "def renorm(x):\n",
        "    return 0.5*x + 0.5\n",
        "\n",
        "def plot(x,renormalize=True):\n",
        "    if renormalize:\n",
        "        plt.imshow(renorm(x).data[0].cpu().numpy(), cmap='gray')\n",
        "    else:\n",
        "        plt.imshow(x.data[0].cpu().numpy(), cmap='gray')\n",
        "\n",
        "\n",
        "exit_window = 50 # number of consecutive MSE values upon which we compare\n",
        "thresh_ratio = 45 # number of MSE values that must be larger for us to exit\n",
        "def exit_check(window, i): # if converged, then exit current experiment\n",
        "    mse_base = window[0] # get first mse value in window\n",
        "    \n",
        "    if len(np.where(window > mse_base)[0]) >= thresh_ratio: # if 20/25 values in window are higher than mse_base\n",
        "        return True, mse_base\n",
        "    else:\n",
        "        mse_last = window[exit_window-1] #get the last value of MSE in window\n",
        "        return False, mse_last\n",
        "\n",
        "\n",
        "def define_compose(NC, IMG_SIZE): # define compose based on NUM_CHANNELS, IMG_SIZE\n",
        "    if NC == 1: #grayscale\n",
        "        compose = transforms.Compose([\n",
        "            transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
        "            transforms.Grayscale(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((.5,.5,.5),(.5,.5,.5))\n",
        "        ])\n",
        "    elif NC == 3: #rgb\n",
        "        compose = transforms.Compose([\n",
        "            transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((.5,.5,.5),(.5,.5,.5))\n",
        "        ])\n",
        "    return compose\n",
        "\n",
        "def set_dtype(CUDA):\n",
        "    if CUDA: # if cuda is available\n",
        "        return torch.cuda.FloatTensor\n",
        "    else:\n",
        "        return torch.FloatTensor\n",
        "\n",
        "def get_path_out(args, path_in):\n",
        "    fn = path_leaf(path_in[0]) # format filename from path\n",
        "\n",
        "    if args.ALG == 'bm3d' or args.ALG == 'tval3':\n",
        "        file_ext = 'mat' # if algorithm is implemented in matlab\n",
        "    else:\n",
        "        file_ext = 'npy' # if algorithm is implemented in python\n",
        "\n",
        "    path_out = 'reconstructions/{0}/{1}/meas{2}/im{3}.{4}'.format( \\\n",
        "            args.DATASET, args.ALG, args.NUM_MEASUREMENTS, fn, file_ext)\n",
        "\n",
        "    full_path = os.getcwd()  + '/' + path_out\n",
        "    return full_path\n",
        "\n",
        "\n",
        "def recons_exists(args, path_in):\n",
        "    path_out = get_path_out(args, path_in)\n",
        "    print(path_out)\n",
        "    if os.path.isfile(path_out):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def save_reconstruction(x_hat, args, path_in):\n",
        "    path_out = get_path_out(args, path_in)\n",
        "\n",
        "    if not os.path.exists(os.path.dirname(path_out)):\n",
        "        try:\n",
        "            os.makedirs(os.path.dirname(path_out))\n",
        "        except OSError as exc: # guard against race condition\n",
        "            if exc.errno != errno.EEXIST:\n",
        "                raise\n",
        "\n",
        "    np.save(path_out, x_hat)\n",
        "\n",
        "def check_args(args): # check args for correctness\n",
        "    IM_DIMN = args.IMG_SIZE * args.IMG_SIZE * args.NUM_CHANNELS\n",
        "\n",
        "    if isinstance(args.NUM_MEASUREMENTS, int):\n",
        "        if args.NUM_MEASUREMENTS > IM_DIMN:\n",
        "            raise ValueError('NUM_MEASUREMENTS must be less than image dimension ' \\\n",
        "                + str(IM_DIMN))\n",
        "    else:\n",
        "        for num_measurements in args.NUM_MEASUREMENTS:\n",
        "            if num_measurements > IM_DIMN:\n",
        "                raise ValueError('NUM_MEASUREMENTS must be less than image dimension ' \\\n",
        "                    + str(IM_DIMN))\n",
        "    if not args.DEMO == 'False':\n",
        "        if not args.DEMO == 'True':\n",
        "            raise ValueError('DEMO must be either True or False.')\n",
        "\n",
        "def convert_to_list(args): # returns list for NUM_MEAS, BATCH\n",
        "    if not isinstance(args.NUM_MEASUREMENTS, list):\n",
        "        NUM_MEASUREMENTS_LIST = [args.NUM_MEASUREMENTS]\n",
        "    else:\n",
        "        NUM_MEASUREMENTS_LIST = args.NUM_MEASUREMENTS\n",
        "    if not isinstance(args.ALG, list):\n",
        "        ALG_LIST = [args.ALG]\n",
        "    else:\n",
        "        ALG_LIST = args.ALG\n",
        "    return NUM_MEASUREMENTS_LIST, ALG_LIST\n",
        "\n",
        "def path_leaf(path):\n",
        "    # if '/' in path and if '\\\\' in path:\n",
        "    #     raise ValueError('Path to image cannot contain both forward and backward slashes')\n",
        "\n",
        "    if '.' in path: # remove file extension\n",
        "        path_no_extn = os.path.splitext(path)[0]\n",
        "    else:\n",
        "        raise ValueError('Filename does not contain extension')\n",
        "    \n",
        "    head, tail = os.path.split(path_no_extn)\n",
        "    return tail or os.path.basename(head)\n",
        "\n",
        "def get_data(args):\n",
        "    compose = define_compose(args.NUM_CHANNELS, args.IMG_SIZE)\n",
        "\n",
        "    if args.DEMO == 'True':\n",
        "        image_direc = 'data/{0}_demo/'.format(args.DATASET)\n",
        "    else:\n",
        "        image_direc = 'data/{0}/'.format(args.DATASET)\n",
        "\n",
        "    dataset = ImageFolderWithPaths(image_direc, transform = compose)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "class ImageFolderWithPaths(datasets.ImageFolder):\n",
        "    \"\"\"Custom dataset that includes image file paths. Extends\n",
        "    torchvision.datasets.ImageFolder\n",
        "    \"\"\"\n",
        "    # override the __getitem__ method. this is the method dataloader calls\n",
        "    def __getitem__(self, index):\n",
        "        # this is what ImageFolder normally returns \n",
        "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
        "        # the image file path\n",
        "        path = self.imgs[index][0]\n",
        "        # make a new tuple that includes original and the path\n",
        "        tuple_with_path = (original_tuple + (path,))\n",
        "\n",
        "        return tuple_with_path      \n"
      ],
      "metadata": {
        "id": "wxmdpA5jAx0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#baselines\n",
        "from sklearn.linear_model import Lasso\n",
        "import scipy.fftpack as fftpack\n",
        "import pywt\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "LMBD = 1e-5\n",
        "\n",
        "def solve_lasso(A_val, y_val, lmbd=1e-1):\n",
        "    num_measurements = y_val.shape[0]\n",
        "    lasso_est = Lasso(alpha=lmbd)#,tol=1e-4,selection='random')\n",
        "    lasso_est.fit(A_val.T, y_val.reshape(num_measurements))\n",
        "    x_hat = lasso_est.coef_\n",
        "    x_hat = np.reshape(x_hat, [-1])\n",
        "    return x_hat\n",
        "\n",
        "def dct2(image_channel):\n",
        "    return fftpack.dct(fftpack.dct(image_channel.T, norm='ortho').T, norm='ortho')\n",
        "\n",
        "def idct2(image_channel):\n",
        "    return fftpack.idct(fftpack.idct(image_channel.T, norm='ortho').T, norm='ortho')\n",
        "\n",
        "def db4(image_channel):\n",
        "    coeffs = pywt.wavedec2(image_channel,'db4')\n",
        "    arr, coeff_slices = pywt.coeffs_to_array(coeffs)\n",
        "    return arr, coeff_slices\n",
        "\n",
        "def idb4(image_channel, coeff_slices):\n",
        "    coeffs_from_arr = pywt.array_to_coeffs(image_channel, coeff_slices, output_format='wavedec2')\n",
        "    return pywt.waverec2(coeffs_from_arr,'db4')\n",
        "\n",
        "def vec(channels,num_channels):\n",
        "    shape = channels[0].shape\n",
        "    image = np.zeros((num_channels, shape[0], shape[1]))\n",
        "    for i, channel in enumerate(channels):\n",
        "        image[i, :, :] = channel\n",
        "    return image.reshape([-1])\n",
        "\n",
        "def devec(vector,num_channels):\n",
        "    size = int(np.sqrt(vector.shape[0]/num_channels))\n",
        "    image = np.reshape(vector, [num_channels, size, size])\n",
        "    channels = [image[i, :, :] for i in range(num_channels)]\n",
        "    return channels\n",
        "\n",
        "def lasso_dct_estimator(args):  #pylint: disable = W0613\n",
        "    \"\"\"LASSO with DCT\"\"\"\n",
        "    def estimator(A_val, y_batch_val, args):\n",
        "        # One can prove that taking 2D DCT of each row of A,\n",
        "        # then solving usual LASSO, and finally taking 2D ICT gives the correct answer.\n",
        "        A_new = copy.deepcopy(A_val)\n",
        "        for i in range(A_val.shape[1]):\n",
        "            A_new[:, i] = vec([dct2(channel) for channel in devec(A_new[:, i],args.NUM_CHANNELS)],args.NUM_CHANNELS)\n",
        "        y_val = y_batch_val[0]\n",
        "        z_hat = solve_lasso(A_new, y_val, LMBD)\n",
        "        x_hat = vec([idct2(channel) for channel in devec(z_hat,args.NUM_CHANNELS)],args.NUM_CHANNELS).T\n",
        "        x_hat = np.maximum(np.minimum(x_hat, 1), -1)\n",
        "        return x_hat\n",
        "    return estimator\n",
        "\n",
        "def lasso_wavelet_estimator(args):  #pylint: disable = W0613\n",
        "    \"\"\"LASSO with DWT\"\"\"\n",
        "    def estimator(A_val, y_batch_val, args):\n",
        "        # One can prove that taking 2D DWT of each row of A,\n",
        "        # then solving usual LASSO, and finally taking 2D IWT gives the correct answer.\n",
        "        A_new = copy.deepcopy(A_val)\n",
        "        arr, coeff_slices = db4(devec(A_new[:,0],args.NUM_CHANNELS)[0])\n",
        "        A_wav = np.zeros((args.NUM_CHANNELS*arr.shape[0]*arr.shape[1],A_val.shape[1]))\n",
        "        for i in range(A_val.shape[1]):\n",
        "            A_wav[:, i] = vec([db4(channel)[0] for channel in devec(A_new[:, i],args.NUM_CHANNELS)],args.NUM_CHANNELS)\n",
        "        y_val = y_batch_val[0]\n",
        "        z_hat = solve_lasso(A_wav, y_val, LMBD)\n",
        "        x_hat = vec([idb4(channel,coeff_slices) for channel in devec(z_hat,args.NUM_CHANNELS)],args.NUM_CHANNELS).T\n",
        "        x_hat = np.maximum(np.minimum(x_hat, 1), -1)\n",
        "        return x_hat\n",
        "    return estimator\n",
        "\n",
        "def get_A(dimension,num_measurements):\n",
        "    return np.sqrt(1.0/num_measurements)*np.random.randn(dimension,num_measurements)"
      ],
      "metadata": {
        "id": "F2ukNtttBEbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cs_dip\n",
        "import numpy as np\n",
        "import parser\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "#import baselines\n",
        "\n",
        "#import utils\n",
        "import time\n",
        "\n",
        "#args = parser.parse_args('configs.json') \n",
        "\n",
        "CUDA = torch.cuda.is_available()\n",
        "dtype = set_dtype(CUDA)\n",
        "se = torch.nn.MSELoss(reduction='none').type(dtype)\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "EXIT_WINDOW = 51\n",
        "loss_re, recons_re = init_output_arrays(args)\n",
        "\n",
        "def dip_estimator(args):\n",
        "    def estimator(A_val, y_batch_val, args):\n",
        "\n",
        "        y = torch.FloatTensor(y_batch_val).type(dtype) # init measurements y\n",
        "        A = torch.FloatTensor(A_val).type(dtype)       # init measurement matrix A\n",
        "\n",
        "        mu, sig_inv, tvc, lrc = utils.get_constants(args, dtype)\n",
        "\n",
        "        for j in range(args.NUM_RESTARTS):\n",
        "            \n",
        "            net = utils.init_dcgan(args)\n",
        "\n",
        "            z = torch.zeros(BATCH_SIZE*args.Z_DIM).type(dtype).view(BATCH_SIZE,args.Z_DIM,1,1)\n",
        "            z.data.normal_().type(dtype) #init random input seed\n",
        "            if CUDA:\n",
        "                net.cuda() # cast network to GPU if available\n",
        "            \n",
        "            optim = torch.optim.RMSprop(net.parameters(),lr=0.001, momentum=0.9, weight_decay=0)\n",
        "            loss_iter = []\n",
        "            recons_iter = [] \n",
        "\n",
        "            for i in range(args.NUM_ITER):\n",
        "\n",
        "                optim.zero_grad()\n",
        "\n",
        "                # calculate measurement loss || y - A*G(z) ||\n",
        "                G = net(z)\n",
        "                AG = torch.matmul(G.view(BATCH_SIZE,-1),A) # A*G(z)\n",
        "                y_loss = torch.mean(torch.sum(se(AG,y),dim=1))\n",
        "\n",
        "                # calculate total variation loss \n",
        "                tv_loss = (torch.sum(torch.abs(G[:,:,:,:-1] - G[:,:,:,1:]))\\\n",
        "                            + torch.sum(torch.abs(G[:,:,:-1,:] - G[:,:,1:,:]))) \n",
        "\n",
        "                # calculate learned regularization loss\n",
        "                layers = net.parameters()\n",
        "                layer_means = torch.cat([layer.mean().view(1) for layer in layers])\n",
        "                lr_loss = torch.matmul(layer_means-mu,torch.matmul(sig_inv,layer_means-mu))\n",
        "                \n",
        "                total_loss = y_loss + lrc*lr_loss + tvc*tv_loss # total loss for iteration i\n",
        "                 \n",
        "                # stopping condition to account for optimizer convergence\n",
        "                if i >= args.NUM_ITER - EXIT_WINDOW: \n",
        "                    recons_iter.append(G.data.cpu().numpy())\n",
        "                    loss_iter.append(total_loss.data.cpu().numpy())\n",
        "                    if i == args.NUM_ITER - 1:\n",
        "                        idx_iter = np.argmin(loss_iter)\n",
        "\n",
        "                total_loss.backward() # backprop\n",
        "                optim.step()\n",
        "\n",
        "            recons_re[j] = recons_iter[idx_iter]       \n",
        "            loss_re[j] = y_loss.data.cpu().numpy()\n",
        "\n",
        "        idx_re = np.argmin(loss_re,axis=0)\n",
        "        x_hat = recons_re[idx_re]\n",
        "\n",
        "        return x_hat\n",
        "\n",
        "    return estimator\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "QRfpic2HA849",
        "outputId": "b566b638-9c20-46dd-9925-f767f809be70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f86f77fed626>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mEXIT_WINDOW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mloss_re\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecons_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_output_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdip_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/compressing_dip/')\n",
        "print(sys.path)\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import os\n",
        "import parser\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "\n",
        "#import utils\n",
        "import cs_dip\n",
        "import baselines as baselines \n",
        "import time\n",
        "\n",
        "NEW_RECONS = False\n",
        "\n",
        "args = parser.parse_args('configs.json')\n",
        "print(args)\n",
        "\n",
        "NUM_MEASUREMENTS_LIST, ALG_LIST = utils.convert_to_list(args)\n",
        "\n",
        "dataloader = utils.get_data(args) # get dataset of images\n",
        "\n",
        "for num_meas in NUM_MEASUREMENTS_LIST:\n",
        "    args.NUM_MEASUREMENTS = num_meas \n",
        "    \n",
        "    # init measurement matrix\n",
        "    A = baselines.get_A(args.IMG_SIZE*args.IMG_SIZE*args.NUM_CHANNELS, args.NUM_MEASUREMENTS)\n",
        "    \n",
        "    for _, (batch, _, im_path) in enumerate(dataloader):\n",
        "\n",
        "        \n",
        "        eta_sig = 0 # set value to induce noise \n",
        "        eta = np.random.normal(0, eta_sig * (1.0 / args.NUM_MEASUREMENTS) ,args.NUM_MEASUREMENTS)\n",
        "        \n",
        "\n",
        "        x = batch.view(1,-1).cpu().numpy() # define image\n",
        "        y = np.dot(x,A) + eta\n",
        "\n",
        "        for alg in ALG_LIST:\n",
        "            args.ALG = alg\n",
        "\n",
        "            if utils.recons_exists(args, im_path): # to avoid redundant reconstructions\n",
        "                continue\n",
        "            NEW_RECONS = True\n",
        "\n",
        "            if alg == 'csdip':\n",
        "                estimator = cs_dip.dip_estimator(args)\n",
        "            elif alg == 'dct':\n",
        "                estimator = baselines.lasso_dct_estimator(args)\n",
        "            elif alg == 'wavelet':\n",
        "                estimator = baselines.lasso_wavelet_estimator(args)\n",
        "            elif alg == 'bm3d' or alg == 'tval3':\n",
        "                raise NotImplementedError('BM3D-AMP and TVAL3 are implemented in Matlab. \\\n",
        "                                            Please see GitHub repository for details.')\n",
        "            else:\n",
        "                raise NotImplementedError\n",
        "\n",
        "            x_hat = estimator(A, y, args)\n",
        "\n",
        "            utils.save_reconstruction(x_hat, args, im_path)\n",
        "\n",
        "if NEW_RECONS == False:\n",
        "    print('Duplicate experiment configurations. No new data generated.')\n",
        "else:\n",
        "    print('Reconstructions generated!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "BNWPgIxh_qEK",
        "outputId": "a2068987-030c-4c2a-9f48-ccf10866d10d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/compressing_dip/', '/content/compressing_dip/', '/content/compressing_dip/', '/content/compressing_dip', '', '/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3c4cea6b6783>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#import utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcs_dip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbaselines\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cs_dip'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}